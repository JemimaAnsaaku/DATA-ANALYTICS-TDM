# Analysing Data Using Stats

<details><summary> Objectives </summary>

Objectives

In this module, I learnt why understanding basic statistics is essential for anyone working with data. At first, I thought visualisations alone could tell the story, but I realised that without knowing the underlying statistics, charts can be misleading or misinterpreted. Statistics is the foundation that helps me make sense of numbers and ensures I can make informed decisions based on data.

Reflection: I now understand that statistics isn’t just about calculations; it’s about thinking critically about what the numbers actually mean and how they connect to the real world.


What I Will Learn

The main goal of this module is to create visualisations in Excel, like charts and graphs, but before I can do that effectively, I need to understand the statistical concepts behind them. It’s like learning the grammar before writing a story. Without the basics, visualisations can look impressive but might tell the wrong story.

Reflection: I realised I often just create charts without checking the data or thinking about what the chart is really showing. This module will help me stop doing that and start using charts to communicate insights clearly.


Why Statistics Matters

I learnt that statistics helps with analysing trends, summarising information, comparing groups, and identifying patterns or anomalies in data. Even simple concepts like averages, percentages, or ranges give context to what I see in a spreadsheet. Understanding statistics also means I can explain my findings confidently to others instead of just showing them numbers.

Reflection: This made me reflect on past work where I might have misread data because I didn’t fully consider the underlying statistics. Knowing the why behind the numbers makes me a more thoughtful analyst.


Visualisations in Excel

Creating graphs or charts in Excel is more than dragging data into a chart type. I learnt that the type of chart I choose must match the kind of data I have and the story I want to tell. For example, using a bar chart for categorical comparisons or a line chart for trends over time. Choosing the wrong chart type can confuse the audience, even if the data is correct.

Reflection: I found this interesting because I’ve seen charts in reports before that looked professional but were actually misleading. Learning the statistical basis for visualisation will help me avoid that mistake.


By the end of this module, I hope to feel more confident interpreting data, applying statistical concepts, and presenting my findings visually. I want to be able to not just make charts, but tell the story behind the numbers. This module is helping me build a mindset where I question, check, and think critically about data before presenting it. Understanding statistics feels like a superpower for making smart, informed decisions with data.

</details>

<details><summary> What are statistics, populations and samples? </summary>


In statistics, a **population** is the entire group I am interested in studying. It’s like imagining all the people, items, or events that could possibly be included in my analysis. For example, if I want to know the average height of students in my school, the population would be every single student in the school. Populations are usually very large, and it’s often not practical to collect data from every member.  

A **sample** is a smaller part of the population that I actually collect data from. The idea is that the sample should represent the population well, so I can make conclusions about the bigger group without having to measure everyone. Using the height example, I might measure just one class of students instead of the whole school.  

Reflection: I realised that understanding the difference between populations and samples is key to making sure my results are meaningful. If I take a sample that isn’t representative, my conclusions could be completely wrong. This made me think about how careful I need to be when choosing who or what I include in my analysis.

Why It Matters

Samples are important because collecting data from a whole population can be expensive, time-consuming, or impossible. By taking a good sample, I can still get accurate insights. I also learnt that statistical formulas, like averages or variances, work differently depending on whether I’m using population data or sample data. For example, the way I calculate standard deviation changes slightly because I’m accounting for the fact I’m estimating from a sample, not measuring the full population.

Reflection: I used to assume that numbers from a small set could just be scaled up to represent the whole group, but now I understand why statisticians adjust calculations when using samples. It’s about reducing bias and making estimates more reliable.

Key Takeaways

- Populations = everyone or everything I want to study.  
- Samples = a smaller group chosen to represent the population.  
- The goal is to use the sample to make informed statements about the population.  
- Choosing a random and representative sample is crucial; otherwise my data could be misleading.  
- Statistical measures can change depending on whether I am working with a population or a sample.  

</details>

<details><summary> descriptive statistics</summary>

Descriptive statistics are about **summarising and describing the data I have**. They don’t try to predict anything or make assumptions about people or things I haven’t measured. It’s like looking at a snapshot of the data and explaining what I see.

For example, if I use a fitness tracker and record my daily steps and heart rate over 10 days, I can use descriptive statistics to say things like: I met my step goal 6 out of 10 days (60%), my highest heart rate was 140 bpm, and my average heart rate was 72 bpm. These numbers summarise what happened in my data without making assumptions about anyone else or what might happen in the future.

Reflection: I realised that descriptive statistics are really useful for **understanding trends and patterns** in data I already have. They help me see the big picture clearly without getting distracted by guesses or predictions.

What They Can Show

Descriptive statistics can tell me:

- How many data points I have in my set  
- The range of values (highest and lowest numbers)  
- How often certain values occur  
- Trends or patterns in the data  

I can show these results using **numbers or charts**, like pie charts, bar charts, or histograms. These visualisations make it easier to understand and communicate what the data is saying.  

Reflection: I noticed that even though descriptive statistics can give me a clear view of the data, they **cannot compare groups, make conclusions, or predict the future**. That’s something I need to remember, because it’s easy to make assumptions if I forget this limitation.

Why It Matters

In the fitness tracker example, even if I see that I only hit my goal 60% of the time, I cannot say I am unhealthy. I also cannot use my 10-day data to predict how other people with similar characteristics will perform. Descriptive statistics are only about **what has already happened in the data I collected**.

Reflection: This helped me understand why data analysts often start with descriptive statistics before doing anything else. It’s the first step to really knowing the data before making any guesses or predictions.

</details>

<details><summary>inferential statistics </summary>

Inferential statistics are about going beyond the data I already have. Instead of just describing my dataset (like with descriptive statistics), I can take a **smaller sample of a population** and then make predictions or draw conclusions about the bigger population it represents.  


For example, if I wanted to know how healthy people in a whole city are, I wouldn’t be able to track every single person. That would take too much time and money. Instead, I could take a smaller group of people (a sample) and study them. Then, using inferential statistics, I could use what I find from the sample to say something about the health of the whole city.  


Reflection: This made me realise that inferential statistics are powerful because they allow me to make conclusions about bigger groups even when I don’t have all the data. At the same time, I also understand why I need to be careful — because the way I choose my sample affects how accurate my conclusions will be.


Key Points  

- Inferential statistics lets me **generalise** from a sample to a larger population.  
- It helps with **testing hypotheses**, like checking whether a pattern I see in a small group might also apply to the wider group.  
- The challenge is making sure the **sample is representative**. If it doesn’t reflect the population properly, then my conclusions could be misleading.  
- Different **sampling techniques** can be used to reduce error and increase confidence. The type of technique depends on the type of data I’m working with.  


Reflection: 

I now see the difference between descriptive and inferential statistics more clearly. Descriptive statistics are about “what is happening in the data I already collected,” while inferential statistics are about “what this data might mean for a larger group.” It feels like moving from just looking at facts to making predictions or testing ideas. That shift is what makes inferential statistics such an important tool for data analysts.  

</details>

<details><summary> Statistics and Big data</summary>


What I Understand  

When working with **big data**, there are special challenges that don’t appear as much with smaller datasets. Descriptive statistics are still useful, because they allow me to summarise what the sample looks like and check the **quality of the data**.  


For example, data in big datasets can come from many sources, and some data points might be **corrupted, missing, or incomplete**. Descriptive statistics can help me figure out how much of the data is reliable and what I might need to remove before analyzing it. Graphs like histograms, pie charts, and bar charts are particularly helpful here, because they give me a quick visual understanding of which data points are valid or problematic.  


Example: If I’m analysing a sample of tweets, some tweets may only have text while others have both text and images. Whether or not I include the tweets with images depends on the question I am trying to answer. If my analysis requires only text data, tweets with images might be considered invalid for that study. Descriptive statistics allow me to spot these issues quickly.  


Inferential statistics are also important in big data analytics because they let me make conclusions beyond the immediate sample. Some common inferential techniques in big data include:  

- **Cluster Analysis** – Helps find groups of observations that are similar to each other. This is useful for segmenting customers, users, or patterns in large datasets.  
- **Association Analysis** – Looks for co-occurrences of values in different variables. For example, which products are often bought together.  
- **Regression Analysis** – Measures and predicts the relationship between one or more variables. For instance, how changes in marketing spend might affect sales numbers.  


Reflection:  

Reading this made me realise how descriptive and inferential statistics work together in big data. Descriptive stats help me **clean and understand the data**, while inferential stats help me **extract meaning, find patterns, and make predictions**. I also see that in big datasets, even small errors or missing data points can have a big effect, which is why checking data quality first is crucial.  

This reinforces for me that statistics isn’t just about numbers — it’s about making sense of messy, real-world data so that the conclusions I draw are meaningful and trustworthy. It also makes me appreciate why data analysts spend so much time preparing data before diving into deeper analysis.

</details> 

<details><summary>Common types of data visualisations </summary>

I’ve learned that there are many ways to show data visually, and choosing the right one is important for understanding what the data is really telling us. To decide, I need to ask myself some key questions:


How many variables do I want to show?


How many data points are in each variable?


Is the data over time, or am I comparing values at one point in time?


Answering these questions helps me pick the visualisation that makes the most sense for my data.


Here are some common chart types and what I understand about them:


Line Charts – Line charts are useful when I want to show trends over time. They are best for continuous data, like daily sales or temperature changes over a week. Each point on the line represents a data value at a specific time, and the line connects the points to show the trend. I’ve realised line charts are very helpful when I need to see patterns, peaks, or drops in data over a period. They also make it easier to compare multiple variables at once, if I add more lines to the same chart.


Column Charts – Column charts show comparisons between categories. I understand that each column represents a category and the height shows the value. They work well when I want to see differences at a glance, like which product sold the most. They are simple to read and easy to understand for most people. They are less effective if I have too many categories because the chart can get crowded.


Bar Charts – Bar charts are similar to column charts but horizontal. I find them useful when category names are long, because it’s easier to read text from left to right than stacked vertically. They are also helpful when comparing a large number of categories, because horizontal bars are easier to fit on a page or screen.


Pie Charts – Pie charts show proportions of a whole. I understand that each slice represents a part of the total, so the size of the slice matters. They are good when I want to highlight percentages, like the share of sales from different regions. However, I’ve learned that too many slices can make it hard to see differences, so pie charts are best for a small number of categories.


Scatter Plots – Scatter plots are used to show the relationship between two variables. Each point represents one observation. I’ve found them very helpful to spot patterns, correlations, or outliers. For example, I can see if higher advertising spend is associated with higher sales. They don’t show proportions or trends over time, but they are great for exploring how variables interact.
From doing this section, I’ve realised that choosing the right chart is not just about making the data look nice. It’s about making the story behind the data clear. A good visualisation helps me, and anyone else looking at the data, to see patterns, comparisons, and trends quickly. It also makes it easier to communicate findings and make decisions based on the data.

</details>

<details><summary>LAB - Practical Create Visualisations in Excel </summary>



Objectives


In this lab, I am learning how to create charts in Microsoft Excel to visualise data.


There are three main parts to this lab:


Part 1: Creating a Line Chart

Part 2: Creating a Column Chart

Part 3: Creating a Pie Chart


The main goal of this lab is to help me understand how visualising data can make it easier to see trends, relationships, and patterns that I might not notice just by looking at the numbers.


I’ve realised that when looking at raw numbers in a dataset, it can be difficult to interpret the data or make decisions from it. Visualising the data helps me see patterns, trends over time, and comparisons across groups.
In this lab, I am using sample datasets to create three different types of charts, and each one teaches me a slightly different way to analyse the data.


Part 1: Creating a Line Chart


The purpose of a line chart is to show trends over time. In this example, I am looking at Profit and Revenue over the years 2017–2021.
Download the data file


I downloaded the sample file Bike Sales_Visualizations_Lab.xlsx to my OneDrive and opened it in Excel.
There are four worksheets, and I will use each one for different visualisations.


Insert the line chart
I selected the Revenue and Profit by Year worksheet.

I selected the data from cells A3 to C8.

From the Insert menu, I chose the Line chart with Markers option.

I could immediately see a chart with the years on the x-axis and the dollar amounts on the y-axis.

Format the chart

I changed the vertical axis to USD currency with zero decimal places to make the numbers clearer.

I added a chart title, “Revenue vs. Profits”.

I renamed the legend items to “Annual Profit” and “Annual Revenue” so the chart was easy to understand at a glance.

I positioned the legend to the right.

I added axis titles: “Year” for the horizontal axis and “US Dollars” for the vertical axis.


Doing this made me realise that formatting is just as important as creating the chart itself. Even a chart with the right data can be confusing if the labels and numbers are not clear.


Part 2: Creating a Column Chart

*inserting chart*
Column charts are best for comparing values across categories. Here, I am looking at Product Revenue by Country.

I selected cells A3 to E10 in the Product Revenue by Country worksheet.

I inserted a stacked column chart.

The x-axis shows the countries, and the y-axis shows the revenue totals in dollars.

*Formatting the chart*

I gave the chart the title “Product Revenue by Country”.

I changed the vertical axis to USD currency with zero decimal places.

I positioned the legend to the right.

I added axis titles: “Country” for the horizontal axis and “US Dollars” for the vertical axis.

This part helped me see that column charts are useful when comparing multiple groups at a single point in time. The stacked columns also made it easier to see which products contributed the most to each country’s revenue.


Part 3: Creating a Pie Chart


Pie charts are helpful for showing proportions. In this part, I am visualising Revenue by Age Group.

*Inserting the pie chart*

I selected cells A3 to B7.

I inserted a 2D Pie chart.

Each slice of the pie represents the revenue share of each age group.


*Formatting the chart*


I added the title “Revenue Comparison by Age Group”.

I moved the legend to the right.

I added data labels to show both the category name and the percentage of total revenue.

This part made me realise that pie charts are great for showing the proportion of a whole, but they can be confusing if there are too many categories. The labels and percentages really help make the chart readable.


While working through this lab, I experimented with changing chart types and options. I noticed that some charts made the data easier to understand, while others made it confusing.

Line charts are best for trends over time.

Column charts are best for comparing groups.

Pie charts are best for showing proportions.

I’ve also realised that formatting, including titles, axis labels, legends, and data labels, is essential. Without these, even a correctly plotted chart can be hard to read.

</details>


<DETAILS><SUMMARY> Outliers and anomalies </SUMMARY>

Before doing any data analysis, I have to clean the data first.


Part of cleaning means checking for outliers (data points that don’t fit with the rest).


These need attention, because they can either be mistakes or they can be important signals in the data.

An outlier = a value that is much higher or much lower than the others.


Example: one data point sitting far away from the rest of the cluster.


Key thing:

Sometimes it’s just an error → if left in, it could mess up the results.


Sometimes it’s real → and could actually tell me something meaningful


### Why investigate anomalies?

If I ignore them, my analysis could become inaccurate.


If I check them properly, I can:


Correct mistakes.


Keep the “true” unusual values that might explain something important.


So, investigating anomalies is really about making sure my results are valid and trustworthy.


With small datasets, it’s usually simple: just sort or filter the numbers and the odd one stands out.


With big datasets, it’s much harder → I need proper tools to find them.


Two common visualisation methods:

Scatter plots → show individual points and make “odd” ones visible.


Box plots → highlight values that sit way outside the normal range.
</DETAILS>

<details><summary> LAB - Interpret Visualisations With Respect to Others </summary>

# OBJECTIVE

I’m using Excel to spot **outliers** (weird values that don’t fit) 

# KEY DEFINITIONS 

**Outlier**  

A value that’s way bigger or way smaller than the rest. Could be a mistake, could be important.

**PivotTable**  

A quick summary table. It groups raw rows so I can see totals by date, country, etc.

**Scatter chart**  

A dot plot. Each dot is a value. Dots far away from the pack = suspicious.

**LARGE / SMALL**  

Excel functions that pull out the biggest or smallest values without me scrolling for ages.

### Objective

This lab is about learning to identify outliers in a dataset using Excel. Outliers are values that are much higher or lower than the rest of the data, and they can either be mistakes or important signals. Understanding them is key because they can dramatically affect analysis, so it’s not just about finding weird numbers — it’s about interpreting what they mean.


Part 1: Examine a Dataset for Outliers


Step 1: Opening the Dataset


I opened the file Bike Sales_Outlier_Lab.xlsx in Excel online.


Reflection: The first step is simple but crucial — if you can’t access the dataset, nothing else matters. Just opening it and seeing the data helps me start thinking about patterns, like which columns might matter for spotting unusually high or low sales.


Step 2: Creating a Pivot Table


I inserted a PivotTable into a new worksheet and included the Date and Order_Quantity fields.
Reflection: Pivot tables are so useful because they summarise data instantly. Instead of scrolling through dozens of rows, I can see total orders per day. It’s easier to notice when one day spikes far above the others. I also realised that without summarising, outliers might hide among normal-looking data points — this step is about clarity and perspective.


Step 3: Sorting to Identify Outliers


Next, I sorted the Sum of Order_Quantity column from highest to lowest.
Reflection: Sorting is the most basic way to flag extremes. Seeing the largest values on top makes it obvious that December 19th had an unusually high order quantity. Even with small datasets, this step immediately shows which data points might need further investigation. The “why” here is about efficiency — this is the first filter to flag anomalies.


Step 4: Using a Scatter Plot


I copied the pivot table data to blank columns, because Excel won’t chart directly from a pivot table, then inserted a scatter plot with Date on the x-axis and Order_Quantity on the y-axis.

Reflection: Visualising the data makes patterns obvious. On the scatter plot, the December 19th data point sticks out like a sore thumb. Scatter plots are especially important when datasets get larger — numbers alone can be overwhelming, but a chart lets me “see” anomalies. This step reinforced that visual thinking is as important as the numbers themselves.


Step 5: Using LARGE and SMALL Functions

I used formulas to extract the highest and lowest values:

=LARGE($E$4:$E$27,1) gives the highest value.

=LARGE($E$4:$E$27,ROW($1:5)) gives the top 5 highest.

=SMALL($E$4:$E$27,ROW($1:6)) gives the lowest 6.


Reflection: Functions like LARGE and SMALL are invaluable for large datasets. Instead of scanning hundreds or thousands of rows, I can instantly find extreme values. This step highlighted the importance of systematic approaches in data analysis — it’s about speed and accuracy when dealing with a lot of information.


**Handling Outliers**
Once outliers are found, I considered how to deal with them. There are two main options:


1. Delete Outliers
Sometimes outliers are just errors. Removing them ensures that these mistakes don’t distort analysis. But it’s important to save a copy of the dataset first to investigate the source. Reflection: I realised that blindly deleting data could hide important patterns, so keeping a backup is a way of being responsible as an analyst.


2. Normalise Outliers
Another approach is to adjust the outlier to be closer to the other values. For example, changing a spike from 43 orders to 20 if the normal maximum is 19. Reflection: This is a more nuanced approach — it keeps the data usable without letting one extreme value dominate. But it requires careful thinking and research to avoid skewing results. I learned that outlier management isn’t just technical; it’s also about judgement and context.


Reflection

Working through this lab made me think differently about outliers. At first, I just wanted to find “weird” numbers, but now I understand that the context is everything. Outliers can represent mistakes, rare events, or hidden insights. Sorting, charting, and using formulas are just tools — the key is understanding why a value stands out and deciding what to do with it. This lab showed me that data cleaning is not a boring pre-step; it’s a critical part of analysis that shapes every conclusion drawn from the dataset.
If you like, I can do the Visualizations Lab next in this same reflective, GitHub-friendly style, so your entire Excel notes set feels cohesive and personal. Do you want me to do that now?

</DETAILS>

<details><summary> VLOOKUP </summary>

### Objective

The goal is to understand how VLOOKUP works and how it can be used for data analysis and cleaning. This lab focuses on finding information in large datasets and identifying duplicates or related data points efficiently.


What is VLOOKUP?


Definition: VLOOKUP stands for “vertical lookup.” It searches for a value in the leftmost column of a table and returns a value from the same row in another column.
Why it matters: In big spreadsheets, manually looking for a specific value is slow and prone to mistakes. VLOOKUP automates this process, making data analysis much faster and more reliable.


Four key components of VLOOKUP:

- Lookup Value – the specific value you want to find.
- Table Array – the range of cells where the function will search.
- Column Index Number – the column in the range containing the value to return.
- Range Lookup – TRUE for approximate match, FALSE for exact match.


Reflection: Understanding these components is crucial. If any part is wrong (e.g., wrong column number or forgetting FALSE for exact match), the function can give incorrect results. This helps me remember that VLOOKUP isn’t just a formula — it’s a logical search tool.


Using VLOOKUP to Look Up a Typed Value

Example: Find a movie budget by typing the movie title.

Formula:

=VLOOKUP(G2, A1:D11, 3, FALSE)

G2 = typed movie title (lookup value)

A1:D11 = table where data is stored

3 = column with the budget

FALSE = exact match required


Reflection: This shows why VLOOKUP is powerful — I can type a movie name once and immediately get the corresponding budget. It also illustrates the importance of exact matching; without FALSE, Excel might return the wrong budget if the list isn’t sorted.


Using VLOOKUP for Data Cleaning

VLOOKUP can also check for duplicates or inconsistencies across lists.

Example: Check if any villain is also listed as a superhero.


Formula:

=VLOOKUP(B2, $A$2:$A$10, 1, FALSE)

B2 = villain name being checked

$A$2:$A$10 = superhero column (absolute reference)

1 = column to return (the same column here)

FALSE = exact match


Reflection: Using absolute references ($) is critical because it ensures that copying the formula down the column doesn’t change the range. This shows me how VLOOKUP can scale across many rows without losing accuracy. The #N/A error naturally indicates that the villain is not in the superhero list, which is a clever way to flag data issues without extra work.


Customizing VLOOKUP Results

VLOOKUP can be combined with IF and ISNA to return more descriptive results:

Formula:

=IF(ISNA(VLOOKUP(B2, $A$2:$A$10, 1, FALSE)), "Unique", "Duplicate")

ISNA checks if VLOOKUP returned an error (#N/A).

IF then returns “Unique” if there’s no match, “Duplicate” if a match exists.

Reflection: This step shows how formulas can be made user-friendly. Instead of seeing cryptic errors, I get meaningful labels. It highlights the importance of not just finding data but interpreting it in a way that makes sense for decision-making or reporting.


### XLOOKUP – An Alternative


XLOOKUP is a newer function that solves some limitations of VLOOKUP:

Can search any column, not just the leftmost.

Defaults to exact match.

Not backwards compatible, so older Excel versions may not support it.


Reflection: XLOOKUP is more flexible and safer in some scenarios, but VLOOKUP remains widely used and reliable for backwards-compatible sheets. It’s helpful to know both so I can adapt based on Excel version constraints.


ending note

Learning VLOOKUP is about more than memorising a formula. It’s about thinking logically about how data is structured: which column to search, what to return, and how to handle missing values. VLOOKUP and its variations are foundational tools for efficiently exploring, validating, and cleaning data. Combining them with IF and ISNA shows how Excel can communicate findings clearly, not just compute numbers.

</details>

<details><summary> LAB- Using VLOOKUP In Data Analysis </summary>

### Objectives

In this lab, I will use the VLOOKUP function in Excel to:


Part 1: Examine a large dataset and understand why search functions are necessary.

Part 2: Apply the VLOOKUP function to retrieve meaningful information from the dataset.

Why this lab matters:

VLOOKUP provides a systematic way of locating and extracting specific information from big datasets. It ensures that searches are both fast and accurate, reducing human error and making data analysis manageable.


Part 1 – Initial Examination of the Dataset


Applying VLOOKUP


Step 1: Select a Result Display Area

To keep results clear and separate from raw data, I created a small input/output area on the right-hand side of the worksheet.

In U3, I typed: Sales_Order# =

In U4: Product =

In U5: Order_Quantity =

Why this matters: Using a dedicated area avoids cluttering the main dataset and creates a consistent “search hub” where queries and results are displayed. This is especially helpful when datasets are very large.


Step 2: Create the VLOOKUP Functions

Formulas created:

In V4:

=VLOOKUP(V3, A2:S753, 13, FALSE)

In V5:

=VLOOKUP(V3, A2:S753, 14, FALSE)


Breaking it down:

V3 - the lookup value (Sales_Order# entered by the user).

A2:S753 - the dataset range covering all rows and columns.

13 - tells Excel to return the Product (column 13 of the dataset).

14 - tells Excel to return the Order_Quantity (column 14).

FALSE - ensures Excel only finds exact matches.


Why this matters: Each part of the function is essential. The column index tells Excel what information to pull out, and the FALSE parameter prevents Excel from returning “close” but incorrect results.


Step 3: Test the VLOOKUP Functions

Entered 000261274 in V3.

V4 returned Road-650 Red 44 (Product).

V5 returned 2 (Order Quantity).


Observation:
If Excel removes leading zeros (e.g., showing 261274 instead of 000261274), typing '000261274 (with an apostrophe) forces Excel to keep the formatting.
Why this matters: Data formatting issues, like dropped leading zeros, can cause lookups to fail. Understanding how Excel handles text vs numbers is crucial for avoiding errors.


Step 4: Test with More Inputs

I tried additional Sales_Order# values. Results consistently returned the correct Product and Order Quantity when valid IDs were used.


Why this matters: Testing across multiple cases builds confidence in the formula’s reliability. It also confirms that the lookup range and column indexes were chosen correctly.


Step 5: Improve the VLOOKUP Function

Problem: If an invalid Sales_Order# (e.g., 12345) is entered, the result is #N/A.

Solution: Wrap the function in IFNA to provide a more user-friendly response:

In V4:

=IFNA(VLOOKUP(V3, A2:S753, 13, FALSE), "Not Found")

In V5:

=IFNA(VLOOKUP(V3, A2:S753, 14, FALSE), "Not Found")


Why this matters: Error messages like #N/A can confuse non-technical users. Returning “Not Found” communicates the same idea clearly and professionally.


</details>
